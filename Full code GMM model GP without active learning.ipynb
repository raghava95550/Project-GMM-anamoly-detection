{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import cv2 as cv\n",
    "import datetime\n",
    "from sklearn import cluster,datasets\n",
    "from scipy.stats import multivariate_normal\n",
    "from sklearn.cluster import KMeans\n",
    "from datetime import datetime, timedelta\n",
    "import csv\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import roc_curve\n",
    "from sklearn.metrics import roc_auc_score\n",
    "import matplotlib.pyplot as plt\n",
    "from numpy.linalg import inv\n",
    "from sklearn.gaussian_process.kernels import RBF\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.gaussian_process import GaussianProcessClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def file_reading(file_path):\n",
    "    #reading annotations file\n",
    "    ann_file = open(file_path,\"r\") #opening file in read mode only\n",
    "    strings = [x.strip() for x in ann_file.readlines()]\n",
    "    stimes=[]\n",
    "    etimes=[]\n",
    "    for i in range(len(strings)):\n",
    "        s1,s2=strings[i].split(\"-\")\n",
    "        stimes.append(s1.strip())\n",
    "        etimes.append(s2.strip())\n",
    "    return stimes,etimes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare(stime,etime,abnormal_stimes,abnormal_etimes):\n",
    "    length = len(abnormal_stimes)\n",
    "    for i in range(length):\n",
    "        t1 = datetime.strptime(abnormal_stimes[i], '%M:%S').time()\n",
    "        t2 = datetime.strptime(abnormal_etimes[i], '%M:%S').time()\n",
    "        obj1 = timedelta(hours=t1.hour, minutes=t1.minute, seconds=t1.second)\n",
    "        obj2 = timedelta(hours=t2.hour, minutes=t2.minute, seconds=t2.second)\n",
    "        if (stime >= obj1 and etime <= obj2) or (stime < obj1 and etime > obj1) or (stime < obj2 and etime > obj2):\n",
    "            return 1\n",
    "    return -1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class labeling_objects:\n",
    "    def __init__(self,clip_no,stime,etime,label):\n",
    "        self.clip_no = clip_no\n",
    "        self.stime = stime\n",
    "        self.etime = etime\n",
    "        self.label = label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def video_input(path,ann_file_path):\n",
    "    cap = cv.VideoCapture(path)\n",
    "    ret, frame1 = cap.read()\n",
    "\n",
    "    #reading first frame\n",
    "    prvs_gray = cv.cvtColor(frame1,cv.COLOR_BGR2GRAY)\n",
    "    width=prvs_gray.shape[1]\n",
    "    height = prvs_gray.shape[0]\n",
    "\n",
    "    prvs = cv.resize(prvs_gray,(int(width/10),int(height/10)),interpolation = cv.INTER_AREA)\n",
    "\n",
    "    #intializing all values\n",
    "    cnt=0\n",
    "    flow_array = [] # to store output from farneback\n",
    "    #mag_list = []\n",
    "    flow_array_array = [] #for storing all clips\n",
    "    secs = 0\n",
    "    clip = 0\n",
    "    label_objects_array = []\n",
    "    abnormal_stimes,abnormal_etimes = file_reading(ann_file_path)\n",
    "    \n",
    "    while(True):\n",
    "        ret, frame2 = cap.read()\n",
    "        cnt=cnt+1\n",
    "        if cnt%50 == 0:\n",
    "            flow_array_array.append(flow_array)\n",
    "            flow_array=[]\n",
    "            clip+=1\n",
    "            #adding labels here after one clip is recorded.\n",
    "            secs = secs+2\n",
    "            stime = timedelta(seconds = secs-2)\n",
    "            etime = timedelta(seconds = secs)\n",
    "            label = compare(stime,etime,abnormal_stimes,abnormal_etimes)\n",
    "            label_objects_array.append(labeling_objects(clip,stime,etime,label))\n",
    "            \n",
    "        if ret==False:\n",
    "            break\n",
    "\n",
    "        #converting frame into gray and resizing \n",
    "        gray2 = cv.cvtColor(frame2,cv.COLOR_BGR2GRAY)\n",
    "        next = cv.resize(gray2,(int(width/10),int(height/10)),interpolation = cv.INTER_AREA)\n",
    "\n",
    "        #calculating optical flow giving two consecutive frames as input\n",
    "        flow = cv.calcOpticalFlowFarneback(prvs,next, None, 0.5, 3, 15, 3, 5, 1.2, 0)\n",
    "\n",
    "        # appending to the flow array\n",
    "        flow_array.append(flow)\n",
    "\n",
    "        #changing current frame as previous frame\n",
    "        prvs = next\n",
    "    if len(flow_array)!= 0:\n",
    "        flow_array_array.append(flow_array)\n",
    "        clip+=1\n",
    "        secs = secs+1\n",
    "        stime = timedelta(seconds = secs-1)\n",
    "        etime = timedelta(seconds = secs)\n",
    "        label = compare(stime,etime,abnormal_stimes,abnormal_etimes)\n",
    "        label_objects_array.append(labeling_objects(clip,stime,etime,label))\n",
    "    flow_array_length = len(flow_array_array)\n",
    "    print(\"Total no of clips\",flow_array_length)\n",
    "    print(\"total frames\",cnt)\n",
    "    \n",
    "    return flow_array_array,label_objects_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GMM:\n",
    "    \n",
    "    def __init__(self,X,k,weights,means,variances,n_iter):\n",
    "        self.X=X\n",
    "        self.k=k\n",
    "        self.weights=weights\n",
    "        self.means = means\n",
    "        self.variances = variances\n",
    "        self.eps=1e-8\n",
    "        self.n_iter = n_iter\n",
    "        \n",
    "    def run(self):\n",
    "        for step in range(self.n_iter):\n",
    "        \n",
    "            likelihood=[]\n",
    "            for j in range(self.k):\n",
    "                likelihood.append(self.pdf(self.X, self.means[j], np.sqrt(self.variances[j])))\n",
    "            likelihood = np.array(likelihood)\n",
    "            \n",
    "            b = []\n",
    "            # Maximization step \n",
    "            \n",
    "            for j in range(self.k):\n",
    "                # use the current values for the parameters to evaluate the posterior\n",
    "                # probabilities of the data to have been generanted by each gaussian    \n",
    "                b.append((likelihood[j] * self.weights[j]) / (np.sum([likelihood[i] * self.weights[i] for i in range(self.k)],axis=0))+self.eps)\n",
    "\n",
    "                # updage mean and variance\n",
    "                self.means[j] = np.sum(b[j] * self.X) / (np.sum(b[j]+self.eps))\n",
    "                self.variances[j] = np.sum(b[j] * np.square(self.X - self.means[j])) / (np.sum(b[j]+self.eps))\n",
    "                \n",
    "                #print(\"b dist=\",b[j])\n",
    "                # update the weights\n",
    "                self.weights[j] = np.mean(b[j])\n",
    "            #print(self.weights)\n",
    "            return self\n",
    "\n",
    "    def pdf(self,data,mean:float,variance:float):\n",
    "        s1 = 1/(np.sqrt(2*np.pi*variance))\n",
    "        s2 = np.exp(-(np.square(data - mean)/(2*variance)))\n",
    "        return s1*s2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Function for distribution formation from the mag array of a clip\n",
    "def func_distribution_formation(arr):\n",
    "    dist_arr =[] \n",
    "    height,width,x =arr[0].shape\n",
    "    for idx in range(int(height)):\n",
    "        for j in range(int(width)):\n",
    "            dist=[]\n",
    "            for i in range(len(arr)):\n",
    "                dist.append(arr[i][idx,j])\n",
    "            dist=np.asarray(dist)\n",
    "            dist_arr.append(dist)\n",
    "    dist_arr=np.asarray(dist_arr)\n",
    "    return dist_arr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#intial parameters for em clustering \n",
    "def intial_parameters(arr):\n",
    "    k=4\n",
    "    arr = np.asarray(arr)\n",
    "    intial_weights = [(1-0.1)/k for i in range(k)]\n",
    "    intial_weights = np.append(intial_weights,0.1) # adding extra weight element for background purpose\n",
    "    means = np.random.choice(arr.flatten(),(k,2))\n",
    "    means =np.append(means,[0,0]) # adding extra mean element for background purpose\n",
    "\n",
    "    cov = np.random.sample(size=k)\n",
    "    cov = np.append(cov,4.0) # adding extra cov element for background purpose\n",
    "    return k,intial_weights,means,cov"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class data_objects:\n",
    "    def __init__(self,clip_no,stime,etime,weights_data,label):\n",
    "        self.clip_no = clip_no\n",
    "        self.stime = stime\n",
    "        self.etime = etime\n",
    "        self.weights_data = weights_data\n",
    "        self.label = label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Function to run video input,distribution formation and input to GMM and get updated weights as output\n",
    "def run(path,ann_file_path):\n",
    "    mag_arr_all_clips,loa = video_input(path,ann_file_path)\n",
    "    k,initial_weights,means,cov = intial_parameters(mag_arr_all_clips[0])\n",
    "    data_object_array=[]\n",
    "    for index in range(len(mag_arr_all_clips)):\n",
    "        updated_weights=[]\n",
    "        updated_means=[]\n",
    "        updated_variances=[]\n",
    "        dist_arr = func_distribution_formation(mag_arr_all_clips[index])\n",
    "        for i in range(dist_arr.shape[0]):\n",
    "            gmm = GMM(dist_arr[i],k+1,initial_weights.copy(),means.copy(),cov.copy(),50)\n",
    "            gmm.run()\n",
    "            updated_weights.append(gmm.weights)\n",
    "            updated_means.append(gmm.means)\n",
    "            updated_variances.append(gmm.variances)\n",
    "        data_object_array.append(data_objects(loa[index].clip_no,loa[index].stime,loa[index].etime,np.asarray(updated_weights).flatten(),loa[index].label))        \n",
    "    return data_object_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#running our entire code.\n",
    "ann_file_path = \"E:\\\\Study\\\\Sem Project\\\\Data\\\\abnormal_times.txt\"\n",
    "path = \"E:\\\\Study\\\\Sem Project\\\\Data\\\\traffic-junction.avi\"\n",
    "total_data_objects = run(path,ann_file_path) #path to video and path to annotations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_roc_curve(fpr, tpr):\n",
    "    plt.plot(fpr, tpr, color='orange', label='ROC')\n",
    "    plt.plot([0, 1], [0, 1], color='darkblue', linestyle='--')\n",
    "    plt.xlabel('False Positive Rate')\n",
    "    plt.ylabel('True Positive Rate')\n",
    "    plt.title('Receiver Operating Characteristic (ROC) Curve')\n",
    "    plt.legend()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#dividing the total dataset into 60% training and 40% test\n",
    "total_no = len(total_data_objects)\n",
    "print(\"total objects=\",total_no)\n",
    "train_no=round(0.6*total_no)\n",
    "print(\"train objects=\",train_no)\n",
    "test_no=round(0.4*total_no)\n",
    "print(\"test objects=\",test_no)\n",
    "\n",
    "X_train=[]\n",
    "X_test=[]\n",
    "Y_train=[]\n",
    "Y_test=[]\n",
    "\n",
    "for i in range(train_no):\n",
    "    X_train.append(total_data_objects[i].weights_data)\n",
    "    Y_train.append(total_data_objects[i].label)\n",
    "for i in range(train_no,total_no):\n",
    "    X_test.append(total_data_objects[i].weights_data)\n",
    "    Y_test.append(total_data_objects[i].label)\n",
    "\n",
    "X_test=np.asarray(X_test)\n",
    "X_train=np.asarray(X_train)\n",
    "#print(\"X_test\",X_test)\n",
    "#print(\"X_train\",X_train)    \n",
    "\n",
    "Y_test=np.asarray(Y_test)\n",
    "Y_train=np.asarray(Y_train)\n",
    "Y_test=Y_test\n",
    "Y_train=Y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kernel = 1.0*RBF(1.0)\n",
    "gpc = GaussianProcessClassifier(kernel=kernel,random_state=0).fit(X_train,Y_train)\n",
    "gpc.score(X_train,Y_train)\n",
    "pred_labels=gpc.predict(X_test)\n",
    "print(\"labels predicted\",len(pred_labels))\n",
    "print(Y_test.reshape(1,-1))\n",
    "print(pred_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#performance\n",
    "print(\"accuracy =\",accuracy_score(pred_labels,Y_test)*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cm = confusion_matrix(pred_labels,Y_test)\n",
    "print(cm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "auc = roc_auc_score(Y_test,pred_labels)\n",
    "print('AUC: %.2f' % auc)\n",
    "fpr, tpr, thresholds = roc_curve(Y_test,pred_labels)\n",
    "plot_roc_curve(fpr, tpr)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
