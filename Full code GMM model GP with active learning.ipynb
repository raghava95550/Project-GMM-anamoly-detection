{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import cv2 as cv\n",
    "import datetime\n",
    "from sklearn import cluster,datasets\n",
    "from scipy.stats import multivariate_normal\n",
    "from sklearn.cluster import KMeans\n",
    "from datetime import datetime, timedelta\n",
    "import csv\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import roc_curve\n",
    "from sklearn.metrics import roc_auc_score\n",
    "import matplotlib.pyplot as plt\n",
    "from numpy.linalg import inv\n",
    "from sklearn.gaussian_process.kernels import RBF\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.gaussian_process import GaussianProcessClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def file_reading(file_path):\n",
    "    #reading annotations file\n",
    "    ann_file = open(file_path,\"r\") #opening file in read mode only\n",
    "    strings = [x.strip() for x in ann_file.readlines()]\n",
    "    stimes=[]\n",
    "    etimes=[]\n",
    "    for i in range(len(strings)):\n",
    "        s1,s2=strings[i].split(\"-\")\n",
    "        stimes.append(s1.strip())\n",
    "        etimes.append(s2.strip())\n",
    "    return stimes,etimes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare(stime,etime,abnormal_stimes,abnormal_etimes):\n",
    "    length = len(abnormal_stimes)\n",
    "    for i in range(length):\n",
    "        t1 = datetime.strptime(abnormal_stimes[i], '%M:%S').time()\n",
    "        t2 = datetime.strptime(abnormal_etimes[i], '%M:%S').time()\n",
    "        obj1 = timedelta(hours=t1.hour, minutes=t1.minute, seconds=t1.second)\n",
    "        obj2 = timedelta(hours=t2.hour, minutes=t2.minute, seconds=t2.second)\n",
    "        if (stime >= obj1 and etime <= obj2) or (stime < obj1 and etime > obj1) or (stime < obj2 and etime > obj2):\n",
    "            return 1\n",
    "    return -1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class labeling_objects:\n",
    "    def __init__(self,clip_no,stime,etime,label):\n",
    "        self.clip_no = clip_no\n",
    "        self.stime = stime\n",
    "        self.etime = etime\n",
    "        self.label = label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def video_input(path,ann_file_path):\n",
    "    cap = cv.VideoCapture(path)\n",
    "    ret, frame1 = cap.read()\n",
    "\n",
    "    #reading first frame\n",
    "    prvs_gray = cv.cvtColor(frame1,cv.COLOR_BGR2GRAY)\n",
    "    width=prvs_gray.shape[1]\n",
    "    height = prvs_gray.shape[0]\n",
    "\n",
    "    prvs = cv.resize(prvs_gray,(int(width/10),int(height/10)),interpolation = cv.INTER_AREA)\n",
    "\n",
    "    #intializing all values\n",
    "    cnt=0\n",
    "    flow_array = [] # to store output from farneback\n",
    "    #mag_list = []\n",
    "    flow_array_array = [] #for storing all clips\n",
    "    secs = 0\n",
    "    clip = 0\n",
    "    label_objects_array = []\n",
    "    abnormal_stimes,abnormal_etimes = file_reading(ann_file_path)\n",
    "    \n",
    "    while(True):\n",
    "        ret, frame2 = cap.read()\n",
    "        cnt=cnt+1\n",
    "        if cnt%50 == 0:\n",
    "            flow_array_array.append(flow_array)\n",
    "            flow_array=[]\n",
    "            clip+=1\n",
    "            #adding labels here after one clip is recorded.\n",
    "            secs = secs+2\n",
    "            stime = timedelta(seconds = secs-2)\n",
    "            etime = timedelta(seconds = secs)\n",
    "            label = compare(stime,etime,abnormal_stimes,abnormal_etimes)\n",
    "            label_objects_array.append(labeling_objects(clip,stime,etime,label))\n",
    "            \n",
    "        if ret==False:\n",
    "            break\n",
    "\n",
    "        #converting frame into gray and resizing \n",
    "        gray2 = cv.cvtColor(frame2,cv.COLOR_BGR2GRAY)\n",
    "        next = cv.resize(gray2,(int(width/10),int(height/10)),interpolation = cv.INTER_AREA)\n",
    "\n",
    "        #calculating optical flow giving two consecutive frames as input\n",
    "        flow = cv.calcOpticalFlowFarneback(prvs,next, None, 0.5, 3, 15, 3, 5, 1.2, 0)\n",
    "\n",
    "        # appending to the flow array\n",
    "        flow_array.append(flow)\n",
    "\n",
    "        #changing current frame as previous frame\n",
    "        prvs = next\n",
    "    if len(flow_array)!= 0:\n",
    "        flow_array_array.append(flow_array)\n",
    "        clip+=1\n",
    "        secs = secs+1\n",
    "        stime = timedelta(seconds = secs-1)\n",
    "        etime = timedelta(seconds = secs)\n",
    "        label = compare(stime,etime,abnormal_stimes,abnormal_etimes)\n",
    "        label_objects_array.append(labeling_objects(clip,stime,etime,label))\n",
    "    flow_array_length = len(flow_array_array)\n",
    "    print(\"Total no of clips\",flow_array_length)\n",
    "    print(\"total frames\",cnt)\n",
    "    \n",
    "    return flow_array_array,label_objects_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GMM:\n",
    "    \n",
    "    def __init__(self,X,k,weights,means,variances,n_iter):\n",
    "        self.X=X\n",
    "        self.k=k\n",
    "        self.weights=weights\n",
    "        self.means = means\n",
    "        self.variances = variances\n",
    "        self.eps=1e-8\n",
    "        self.n_iter = n_iter\n",
    "        \n",
    "    def run(self):\n",
    "        for step in range(self.n_iter):\n",
    "        \n",
    "            likelihood=[]\n",
    "            for j in range(self.k):\n",
    "                likelihood.append(self.pdf(self.X, self.means[j], np.sqrt(self.variances[j])))\n",
    "            likelihood = np.array(likelihood)\n",
    "            \n",
    "            b = []\n",
    "            # Maximization step \n",
    "            \n",
    "            for j in range(self.k):\n",
    "                # use the current values for the parameters to evaluate the posterior\n",
    "                # probabilities of the data to have been generanted by each gaussian    \n",
    "                b.append((likelihood[j] * self.weights[j]) / (np.sum([likelihood[i] * self.weights[i] for i in range(self.k)],axis=0))+self.eps)\n",
    "\n",
    "                # updage mean and variance\n",
    "                self.means[j] = np.sum(b[j] * self.X) / (np.sum(b[j]+self.eps))\n",
    "                self.variances[j] = np.sum(b[j] * np.square(self.X - self.means[j])) / (np.sum(b[j]+self.eps))\n",
    "                \n",
    "                #print(\"b dist=\",b[j])\n",
    "                # update the weights\n",
    "                self.weights[j] = np.mean(b[j])\n",
    "            #print(self.weights)\n",
    "            return self\n",
    "\n",
    "    def pdf(self,data,mean:float,variance:float):\n",
    "        s1 = 1/(np.sqrt(2*np.pi*variance))\n",
    "        s2 = np.exp(-(np.square(data - mean)/(2*variance)))\n",
    "        return s1*s2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Function for distribution formation from the mag array of a clip\n",
    "def func_distribution_formation(arr):\n",
    "    dist_arr =[] \n",
    "    height,width,x =arr[0].shape\n",
    "    for idx in range(int(height)):\n",
    "        for j in range(int(width)):\n",
    "            dist=[]\n",
    "            for i in range(len(arr)):\n",
    "                dist.append(arr[i][idx,j])\n",
    "            dist=np.asarray(dist)\n",
    "            dist_arr.append(dist)\n",
    "    dist_arr=np.asarray(dist_arr)\n",
    "    return dist_arr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#intial parameters for em clustering \n",
    "def intial_parameters(arr):\n",
    "    k=4\n",
    "    arr = np.asarray(arr)\n",
    "    intial_weights = [(1-0.1)/k for i in range(k)]\n",
    "    intial_weights = np.append(intial_weights,0.1) # adding extra weight element for background purpose\n",
    "    means = np.random.choice(arr.flatten(),(k,2))\n",
    "    means =np.append(means,[0,0]) # adding extra mean element for background purpose\n",
    "\n",
    "    cov = np.random.sample(size=k)\n",
    "    cov = np.append(cov,4.0) # adding extra cov element for background purpose\n",
    "    return k,intial_weights,means,cov"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class data_objects:\n",
    "    def __init__(self,clip_no,stime,etime,weights_data,label):\n",
    "        self.clip_no = clip_no\n",
    "        self.stime = stime\n",
    "        self.etime = etime\n",
    "        self.weights_data = weights_data\n",
    "        self.label = label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Function to run video input,distribution formation and input to GMM and get updated weights as output\n",
    "def run(path,ann_file_path):\n",
    "    mag_arr_all_clips,loa = video_input(path,ann_file_path)\n",
    "    k,initial_weights,means,cov = intial_parameters(mag_arr_all_clips[0])\n",
    "    data_object_array=[]\n",
    "    for index in range(len(mag_arr_all_clips)):\n",
    "        updated_weights=[]\n",
    "        updated_means=[]\n",
    "        updated_variances=[]\n",
    "        dist_arr = func_distribution_formation(mag_arr_all_clips[index])\n",
    "        for i in range(dist_arr.shape[0]):\n",
    "            gmm = GMM(dist_arr[i],k+1,initial_weights.copy(),means.copy(),cov.copy(),50)\n",
    "            gmm.run()\n",
    "            updated_weights.append(gmm.weights)\n",
    "            updated_means.append(gmm.means)\n",
    "            updated_variances.append(gmm.variances)\n",
    "        data_object_array.append(data_objects(loa[index].clip_no,loa[index].stime,loa[index].etime,np.asarray(updated_weights).flatten(),loa[index].label))        \n",
    "    return data_object_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#running our entire code.\n",
    "ann_file_path = \"E:\\\\Study\\\\Sem Project\\\\Data\\\\abnormal_times.txt\"\n",
    "path = \"E:\\\\Study\\\\Sem Project\\\\Data\\\\traffic-junction_3.avi\"\n",
    "total_data_objects = run(path,ann_file_path) #path to video and path to annotations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_roc_curve(fpr, tpr):\n",
    "    plt.plot(fpr, tpr, color='orange', label='ROC')\n",
    "    plt.plot([0, 1], [0, 1], color='darkblue', linestyle='--')\n",
    "    plt.xlabel('False Positive Rate')\n",
    "    plt.ylabel('True Positive Rate')\n",
    "    plt.title('Receiver Operating Characteristic (ROC) Curve')\n",
    "    plt.legend()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def kernel(X1, X2, l=1.0, sigma_f=1.0):\n",
    "    sqdist = np.sum(X1**2, 1).reshape(-1, 1) + np.sum(X2**2, 1) - 2 * np.dot(X1, X2.T)\n",
    "    return sigma_f**2 * np.exp(-0.5 / l**2 * sqdist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Guassian process regression\n",
    "#Input:X1: Array of m points (m x d),X2: Array of n points (n x d).\n",
    "#Output: Covariance matrix (m x n).\n",
    "\n",
    "def posterior_predictive(X_s, X_train, Y_train, l=1.0, sigma_f=1.0, sigma_y=1e-8):\n",
    "    #equation(6)\n",
    "    K = kernel(X_train, X_train, l, sigma_f) + sigma_y**2 * np.eye(len(X_train))\n",
    "    K_s = kernel(X_train, X_s, l, sigma_f)\n",
    "    K_ss = kernel(X_s, X_s, l, sigma_f) + 1e-8 * np.eye(len(X_s))\n",
    "    K_inv = inv(K)\n",
    "    #equation(4)\n",
    "    mu_s = K_s.T.dot(K_inv).dot(Y_train)\n",
    "    #equation(5)\n",
    "    cov_s = K_ss - K_s.T.dot(K_inv).dot(K_s)    \n",
    "    return mu_s, cov_s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t_rel = 1 # threshold for q_relative criteria\n",
    "kern = 1.0*RBF(1.0)\n",
    "auc=[]\n",
    "ffpr=[]\n",
    "ttpr=[]\n",
    "\n",
    "for j in range(10):\n",
    "    gpc = GaussianProcessClassifier(kernel=kern,random_state=0)\n",
    "\n",
    "    #dividing the total dataset into 60% training and 40% test\n",
    "    total_no = len(total_data_objects)\n",
    "    print(\"total objects=\",total_no)\n",
    "    train_no=round(0.6*total_no)\n",
    "    print(\"train objects=\",train_no)\n",
    "    test_no=round(0.4*total_no)\n",
    "    print(\"test objects=\",test_no)\n",
    "\n",
    "    X_test=[]\n",
    "    Y_test=[]\n",
    "    train_objects= []\n",
    "\n",
    "    #for taking two normal and one abnormal as new train to GPClassifier.\n",
    "    feature_train = []\n",
    "    label_train = []\n",
    "    ab=0\n",
    "    n=0\n",
    "    \n",
    "    np.random.shuffle(total_data_objects)\n",
    "    for i in range(train_no):\n",
    "        if total_data_objects[i].label == -1 and n < 2:\n",
    "            feature_train.append(total_data_objects[i].weights_data)\n",
    "            label_train.append(total_data_objects[i].label)\n",
    "            n+=1\n",
    "        elif total_data_objects[i].label == 1 and ab < 1:\n",
    "            feature_train.append(total_data_objects[i].weights_data)\n",
    "            label_train.append(total_data_objects[i].label)\n",
    "            ab+=1\n",
    "        else :\n",
    "            train_objects.append(total_data_objects[i])\n",
    "\n",
    "    for i in range(train_no,total_no):\n",
    "        X_test.append(total_data_objects[i].weights_data)\n",
    "        Y_test.append(total_data_objects[i].label)\n",
    "\n",
    "    feature_train = np.asarray(feature_train)\n",
    "    label_train = np.asarray(label_train).reshape(-1,1)\n",
    "    X_test=np.asarray(X_test)\n",
    "    Y_test=np.asarray(Y_test)\n",
    "\n",
    "    print(\"initial feature length\",len(feature_train))\n",
    "    for val in range(10): \n",
    "        q=0\n",
    "        new_objects = []\n",
    "        # training the objects\n",
    "        for i in range(len(train_objects)):\n",
    "            mu_s,cov_s = posterior_predictive(train_objects[i].weights_data.reshape(1,len(train_objects[i].weights_data)), feature_train, label_train) #giving input to gp regression\n",
    "\n",
    "            q_rel = min(2*abs(mu_s),2/abs(np.sqrt(cov_s)))  # calculating q rel criteria\n",
    "            if q_rel < t_rel:    # if q_rel is less than given threshold then give the clip to domain expert\n",
    "                q+=1\n",
    "                '''\n",
    "                print(\"enter label for clip no: \",train_objects[i].clip_no,\"with start time \",train_objects[i].stime,\" end time \",train_objects[i].etime)\n",
    "                ll = int(input())   # taking input from user(domain_expert)\n",
    "                '''\n",
    "\n",
    "                ll=train_objects[i].label\n",
    "                feature_train = np.vstack([feature_train,train_objects[i].weights_data]) #adding newly labeled sample to training features\n",
    "                label_train = np.append(label_train,ll)  # adding new label to training labels\n",
    "            else:\n",
    "                new_objects.append(train_objects[i])\n",
    "\n",
    "        gpc.fit(feature_train,label_train)\n",
    "        pred_labels=gpc.predict(X_test)\n",
    "        train_objects.clear()\n",
    "        print(\"total no of queries \",q)\n",
    "        train_objects = new_objects.copy()\n",
    "        print(\"feature length\",len(feature_train))\n",
    "        print(\"train length\",len(train_objects))\n",
    "\n",
    "        #performance\n",
    "        print(\"accuracy =\",accuracy_score(pred_labels,Y_test)*100)\n",
    "        print(\"confusion matrix\",confusion_matrix(pred_labels,Y_test))\n",
    "\n",
    "    auc.append(roc_auc_score(Y_test,pred_labels))\n",
    "    print('AUC:',auc[j])\n",
    "    fpr, tpr, thresholds = roc_curve(Y_test,pred_labels)\n",
    "    ffpr.append(fpr)\n",
    "    ttpr.append(tpr)\n",
    "    plot_roc_curve(fpr, tpr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"avg auc score\",np.mean(auc))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
